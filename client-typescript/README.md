# Cliente - Gambiarra LLM Club Arena

CLI para conectar participantes ao servidor e rodar LLMs locais.

## InstalaÃ§Ã£o

```bash
pnpm install
pnpm build
```

## Uso

### Cliente com Ollama

```bash
pnpm dev -- \
  --url ws://localhost:3000/ws \
  --pin 123456 \
  --participant-id meu-pc \
  --nickname "Meu Nome" \
  --runner ollama \
  --model llama3.1:8b
```

### Cliente com LM Studio

```bash
pnpm dev -- \
  --url ws://localhost:3000/ws \
  --pin 123456 \
  --participant-id laptop-xyz \
  --nickname "JoÃ£o" \
  --runner lmstudio \
  --model mistral-7b \
  --lmstudio-url http://localhost:1234
```

### Cliente Simulado (Mock)

```bash
pnpm dev -- \
  --url ws://localhost:3000/ws \
  --pin 123456 \
  --participant-id simulador \
  --nickname "Bot Teste" \
  --runner mock
```

## OpÃ§Ãµes CLI

```
--url <url>              WebSocket server URL (default: ws://localhost:3000/ws)
--pin <pin>              Session PIN (required)
--participant-id <id>    Unique participant ID (required)
--nickname <name>        Display name (required)
--runner <type>          Runner: ollama, lmstudio, mock (default: ollama)
--model <model>          Model name (default: llama3.1:8b)
--temperature <temp>     Temperature (default: 0.8)
--max-tokens <tokens>    Max tokens (default: 400)
--ollama-url <url>       Ollama API URL (default: http://localhost:11434)
--lmstudio-url <url>     LM Studio API URL (default: http://localhost:1234)
```

## Runners

### Ollama

Certifique-se de que o Ollama estÃ¡ rodando:

```bash
# Verificar
curl http://localhost:11434/api/tags

# Iniciar (se necessÃ¡rio)
ollama serve

# Baixar modelo
ollama pull llama3.1:8b
```

### LM Studio

1. Abra LM Studio
2. Carregue um modelo
3. VÃ¡ em "Local Server" â†’ "Start Server"
4. Porta padrÃ£o: 1234

### Mock

Gera tokens sintÃ©ticos sem necessidade de LLM real. Ãštil para:
- Testes
- Desenvolvimento
- Ensaios sem hardware

## SimulaÃ§Ã£o em Massa

Para testar com mÃºltiplos clientes:

```bash
# Inicia 5 clientes simulados
pnpm simulate

# Customizar
SERVER_URL=ws://192.168.1.100:3000/ws PIN=999999 pnpm simulate
```

## Funcionamento

1. Cliente conecta ao WebSocket do servidor
2. Envia mensagem `register` com PIN
3. Aguarda mensagem `challenge` do servidor
4. Executa runner local (Ollama/LM Studio/Mock)
5. Streaming de tokens via mensagens `token` com seq incremental
6. Envia `complete` com mÃ©tricas ao final

## ReconexÃ£o

O cliente tenta reconectar automaticamente com backoff exponencial:
- MÃ¡ximo 5 tentativas
- Delay inicial: 1s
- Delay mÃ¡ximo: ~32s

## Exemplo de Output

```
ðŸŽ® Gambiarra LLM Club Client

Using Ollama at http://localhost:11434
âœ“ Runner connection OK

âœ“ Connected to server

ðŸ“¢ New Challenge - Round 1
Prompt: Escreva uma poesia sobre IA
Max tokens: 400, Deadline: 90000ms

.................................................................................
.................................................................................

âœ“ Completed 312 tokens in 54.23s
  First token latency: 850ms
```

## Desenvolvimento

Estrutura:

```
src/
â”œâ”€â”€ cli.ts          # Entry point CLI
â”œâ”€â”€ net/
â”‚   â””â”€â”€ ws.ts       # WebSocket client
â”œâ”€â”€ runners/
â”‚   â”œâ”€â”€ types.ts    # Interface Runner
â”‚   â”œâ”€â”€ ollama.ts   # IntegraÃ§Ã£o Ollama
â”‚   â”œâ”€â”€ lmstudio.ts # IntegraÃ§Ã£o LM Studio
â”‚   â””â”€â”€ mock.ts     # Gerador simulado
â””â”€â”€ scripts/
    â””â”€â”€ simulate.ts # SimulaÃ§Ã£o em massa
```

## Troubleshooting

**Erro "Runner not available":**
- Verifique se Ollama/LM Studio estÃ¡ rodando
- Confirme a porta correta

**Erro "Invalid PIN":**
- Solicite o PIN ao organizador do evento
- Use `curl -X POST http://SERVER:3000/session` se vocÃª Ã© o organizador

**Tokens nÃ£o aparecem no telÃ£o:**
- Verifique conectividade de rede
- Confirme que a rodada foi iniciada no servidor
- Use `--runner mock` para descartar problemas com LLM local
